"""
Cliente para Google Gemini AI - Integraci贸n con FiscAI
"""
import asyncio
from typing import List, Dict, Any, Optional
import google.generativeai as genai
from .config import config

# Configurar Gemini
if config.GEMINI_API_KEY:
    genai.configure(api_key=config.GEMINI_API_KEY)
else:
    raise ValueError("GEMINI_API_KEY no est谩 configurada")

# System Prompt con detecci贸n de ubicaciones
SYSTEM_PROMPT = """
Eres Juan Pablo, un asistente fiscal experto en M茅xico especializado en ayudar a micro y peque帽os negocios.

**CAPACIDADES ESPECIALES:**

1. **Ubicaciones de Bancos y SAT:**
   - Cuando el usuario pregunte sobre d贸nde encontrar un banco Banorte o una oficina del SAT
   - USA la herramienta 'open_map_location' para abrir el mapa
   - Ejemplos de preguntas que deben activar el mapa:
     * "驴D贸nde hay un Banorte?"
     * "驴D贸nde est谩 el SAT m谩s cercano?"
     * "Necesito ir a un banco"
     * "Mu茅strame oficinas del SAT"
     * "Busca un Banorte en Reforma"
     * "驴Hay alguna oficina del SAT cerca?"
   
2. **Asesor铆a Fiscal:**
   - Proporciona informaci贸n sobre r茅gimen fiscal, obligaciones, tr谩mites
   - USA la herramienta 'get_fiscal_advice' para consultas de formalizaci贸n
   
3. **An谩lisis de Riesgo:**
   - Eval煤a la situaci贸n fiscal del usuario
   - USA la herramienta 'analyze_fiscal_risk' cuando pregunten sobre su nivel de cumplimiento

**FORMATO DE RESPUESTA PARA UBICACIONES:**
Cuando uses 'open_map_location', tu respuesta debe ser breve y clara:
- "隆Claro! Te abro el mapa con los Banorte m谩s cercanos."
- "Perfecto, te muestro las oficinas del SAT en tu zona."
- "Busco Banorte en Reforma para ti."

**NO incluyas las coordenadas o detalles t茅cnicos en tu respuesta, eso lo maneja el mapa autom谩ticamente.**

**Tono:** Cercano, profesional pero amigable, como un asesor de confianza.
"""

def detect_user_intent(message: str) -> Dict[str, Any]:
    """
    Detecta la intenci贸n del usuario antes de llamar a Gemini
    para optimizar el uso de herramientas
    """
    message_lower = message.lower()
    
    # Palabras clave para b煤squeda de ubicaciones
    location_keywords = {
        'bank': ['banorte', 'banco', 'sucursal bancaria', 'ir al banco', 'sucursal'],
        'sat': ['sat', 'oficina del sat', 'servicio de administraci贸n tributaria', 
                'centro tributario', 'm贸dulo de atenci贸n', 'oficina tributaria']
    }
    
    # Verbos que indican b煤squeda de ubicaci贸n
    location_verbs = ['d贸nde', 'donde', 'ubica', 'encuentra', 'busca', 'hay', 
                      'mostrar', 'muestra', 'llevar', 'ir', 'cerca', 'cercano',
                      'necesito ir', 'quiero ir', 'c贸mo llegar']
    
    # Detectar tipo de ubicaci贸n
    location_type = None
    if any(keyword in message_lower for keyword in location_keywords['bank']):
        location_type = 'bank'
    elif any(keyword in message_lower for keyword in location_keywords['sat']):
        location_type = 'sat'
    
    # Detectar si es una pregunta de ubicaci贸n
    is_location_query = any(verb in message_lower for verb in location_verbs)
    
    # Extraer posible query espec铆fica (nombre de lugar)
    search_query = None
    location_indicators = [' en ', ' de ', ' cerca de ', ' por ']
    for indicator in location_indicators:
        if indicator in message_lower:
            parts = message_lower.split(indicator, 1)
            if len(parts) > 1:
                # Extraer hasta el siguiente espacio o final
                potential_query = parts[1].split('.')[0].split(',')[0].split('?')[0].strip()
                # Solo si no es muy corto
                if len(potential_query) > 2:
                    search_query = potential_query
                    break
    
    return {
        'is_location_query': is_location_query and location_type is not None,
        'location_type': location_type,
        'search_query': search_query,
        'requires_map': is_location_query and location_type is not None
    }

class GeminiClient:
    """Cliente para interactuar con Google Gemini AI"""
    
    def __init__(self):
        self.model = genai.GenerativeModel(config.GEMINI_MODEL)
    
    async def generate_text(self, prompt: str) -> str:
        """
        Genera texto simple usando Gemini
        
        Args:
            prompt: Prompt para generar texto
            
        Returns:
            Texto generado por Gemini
        """
        try:
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            return response.text if response.text else "No se pudo generar respuesta"
        except Exception as error:
            print(f"Error generando texto: {error}")
            raise error
    
    async def extract_text_from_image(self, image_data: bytes, mime_type: str = "image/jpeg") -> str:
        """
        Extrae texto de una imagen usando OCR de Gemini Vision
        
        Args:
            image_data: Bytes de la imagen
            mime_type: Tipo MIME de la imagen (image/jpeg, image/png, etc.)
            
        Returns:
            Texto extra铆do de la imagen
        """
        try:
            from PIL import Image
            import io
            
            # Convertir bytes a imagen PIL
            image = Image.open(io.BytesIO(image_data))
            
            # Crear prompt para extracci贸n de texto
            prompt = """Extrae TODO el texto visible en esta imagen.

INSTRUCCIONES:
- Transcribe exactamente lo que ves, palabra por palabra
- Mant茅n la estructura original (p谩rrafos, listas, t铆tulos)
- Si hay f贸rmulas matem谩ticas, escr铆belas en formato LaTeX
- Si hay tablas, mant茅n su estructura
- Si hay diagramas, describe su contenido brevemente
- NO agregues interpretaciones, solo transcribe el texto
- Si no hay texto legible, responde "NO_TEXT_FOUND"

Responde SOLO con el texto extra铆do, sin comentarios adicionales."""

            # Usar el modelo para procesar la imagen
            response = await asyncio.to_thread(
                self.model.generate_content,
                [prompt, image]
            )
            
            extracted_text = response.text.strip()
            
            # Validar que se extrajo texto
            if not extracted_text or extracted_text == "NO_TEXT_FOUND":
                raise ValueError("No se pudo extraer texto de la imagen")
            
            return extracted_text
            
        except Exception as error:
            print(f"Error extrayendo texto de imagen: {error}")
            raise error
    
    async def generate_embedding(self, text: str) -> List[float]:
        """
        Genera embedding para un texto usando Gemini
        Compatible con el formato del c贸digo Python original
        
        Args:
            text: Texto para generar embedding
            
        Returns:
            Lista de n煤meros representando el embedding
        """
        try:
            result = await asyncio.to_thread(
                genai.embed_content,
                model=config.GEMINI_EMBED_MODEL,
                content=text,
                task_type="RETRIEVAL_QUERY",  # May煤sculas como en simulate_recomendation.py
                output_dimensionality=config.EMBED_DIM
            )
            
            # Extraer embedding seg煤n la estructura de respuesta
            if isinstance(result, dict):
                if "embedding" in result:
                    emb = result["embedding"]
                    if isinstance(emb, dict) and "values" in emb:
                        return emb["values"]
                    if isinstance(emb, list):
                        return emb
                if "embeddings" in result and isinstance(result["embeddings"], list) and result["embeddings"]:
                    e0 = result["embeddings"][0]
                    if isinstance(e0, dict) and "values" in e0:
                        return e0["values"]
                    if isinstance(e0, list):
                        return e0
            
            # Si result tiene atributo embedding
            if hasattr(result, "embedding"):
                emb = getattr(result, "embedding")
                if isinstance(emb, dict) and "values" in emb:
                    return emb["values"]
                if hasattr(emb, "values"):
                    return emb.values
                if isinstance(emb, list):
                    return emb
            
            # Fallback
            if hasattr(result, "embeddings"):
                emb_list = getattr(result, "embeddings") or []
                if emb_list:
                    e0 = emb_list[0]
                    if isinstance(e0, dict) and "values" in e0:
                        return e0["values"]
                    if hasattr(e0, "values"):
                        return e0.values
                    if isinstance(e0, list):
                        return e0
            
            raise RuntimeError("No se pudo extraer embedding de la respuesta")
            
        except Exception as error:
            print(f"Error generando embedding: {error}")
            raise error
    
    async def generate_recommendation(
        self, 
        profile: Dict[str, Any], 
        context: str
    ) -> str:
        """
        Genera recomendaci贸n fiscal usando RAG (contexto de documentos relevantes)
        Usa el mismo prompt que simulate_recomendation.py que funciona
        
        Args:
            profile: Perfil fiscal del usuario
            context: Contexto construido de documentos relevantes
            
        Returns:
            Recomendaci贸n detallada en formato markdown
        """
        try:
            import json
            
            system_instruction = (
                "Eres un contador experto en M茅xico. "
                "Responde SOLO con el CONTEXTO provisto. Si no es suficiente, indica claramente "
                "'Informaci贸n insuficiente en la base'. Usa lenguaje claro y profesional."
            )
            
            user_prompt = f"""
PERFIL:
{json.dumps(profile, ensure_ascii=False, indent=2)}

TAREA:
Como contador en M茅xico, necesito analizar este perfil y sugerir:

1) **R茅gimen fiscal m谩s conveniente:**
   - Identifica el r茅gimen fiscal 贸ptimo para este perfil
   - Explica brevemente por qu茅 es el m谩s adecuado
   - Menciona alternativas si aplican

2) **Pasos espec铆ficos de formalizaci贸n:**
   - Lista los pasos concretos para formalizarse (RFC, e.firma, CFDI, declaraciones)
   - Indica el orden recomendado
   - Menciona requisitos y documentos necesarios

3) **Fuentes oficiales del SAT consultadas:**
   - Lista las fuentes utilizadas con formato: T铆tulo -> URL
   - Usa solo las fuentes del CONTEXTO provisto
   - Cita las secciones relevantes

CONTEXTO:
{context}
""".strip()

            # Crear modelo con system instruction (igual que simulate_recomendation.py)
            model = genai.GenerativeModel(
                model_name=config.GEMINI_MODEL,
                system_instruction=system_instruction,
                generation_config={
                    "temperature": 0.3,
                    "max_output_tokens": 1200
                }
            )
            
            # Generar contenido
            response = await asyncio.to_thread(
                model.generate_content,
                user_prompt
            )
            
            return response.text or "(Sin texto)"
            
        except Exception as error:
            print(f"Error generando recomendaci贸n RAG: {error}")
            import traceback
            traceback.print_exc()
            raise error
    
    async def enhance_recommendation(
        self, 
        lambda_response: Dict[str, Any], 
        similar_cases: List[Dict[str, Any]], 
        user_context: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Enriquecer recomendaci贸n fiscal con Gemini
        
        Args:
            lambda_response: Respuesta base del sistema
            similar_cases: Casos similares encontrados
            user_context: Contexto del usuario
            
        Returns:
            Recomendaci贸n enriquecida
        """
        try:
            prompt = f"""
Eres un experto asesor fiscal mexicano. Bas谩ndote en la siguiente informaci贸n, genera una recomendaci贸n personalizada y detallada:

**Recomendaci贸n Base:**
{lambda_response.get('recommendation', lambda_response)}

**Perfil del Usuario:**
{lambda_response.get('profile', {})}

**Casos Similares (para referencia):**
{similar_cases[:3]}

{f"**Contexto del Usuario:**\n{user_context}" if user_context else ""}

**Instrucciones:**
1. Mant茅n el formato de la recomendaci贸n original con sus secciones (R茅gimen Fiscal, Pasos, Checklist, etc.)
2. Usa texto en **negrita** para t铆tulos importantes
3. Usa *cursiva* para notas adicionales
4. Mant茅n los bullets y numeraci贸n
5. Aseg煤rate de incluir las fuentes al final
6. S茅 espec铆fico con montos, fechas y requisitos
7. Usa lenguaje claro y accesible para micro-negocios

Responde SOLO con la recomendaci贸n mejorada, sin comentarios adicionales.
"""

            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            enhanced_text = response.text
            
            return {
                **lambda_response,
                'recommendation': enhanced_text,
                'enhanced': True,
                'enhanced_at': asyncio.get_event_loop().time()
            }
            
        except Exception as error:
            print(f"Error enriqueciendo recomendaci贸n: {error}")
            # Si falla Gemini, devolver la respuesta original
            return lambda_response
    
    async def chat_with_assistant(
        self,
        message: str,
        user_context: Optional[Dict[str, Any]] = None,
        chat_history: List[Dict[str, Any]] = None,
        relevant_docs: List[Dict[str, Any]] = None
    ) -> str:
        """
        Chat con el asistente fiscal usando Gemini con detecci贸n autom谩tica de intenciones
        
        Args:
            message: Mensaje del usuario
            user_context: Contexto del usuario
            chat_history: Historial de conversaci贸n
            relevant_docs: Documentos relevantes
            
        Returns:
            Respuesta del asistente en formato JSON con texto, deep_link y tool_used
        """
        try:
            # 1. DETECCIN AUTOMTICA DE INTENCIONES
            intent = detect_user_intent(message)
            
            # 2. SI ES UNA CONSULTA DE UBICACIN, GENERAR RESPUESTA DIRECTAMENTE
            if intent['requires_map']:
                print(f"[CHAT] Detecci贸n autom谩tica: requiere mapa tipo={intent['location_type']}")
                
                # Construir deep link directamente (sin llamar a la herramienta decorada)
                base_url = "fiscai://map"
                params = [f"type={intent['location_type']}"]
                
                if intent['search_query']:
                    params.append(f"query={intent['search_query']}")
                
                deep_link = f"{base_url}?{'&'.join(params)}"
                
                # Construir mensaje descriptivo
                location_name = "Banorte" if intent['location_type'] == "bank" else "oficinas del SAT"
                
                if intent['search_query']:
                    message_text = f" Busco {location_name} en {intent['search_query']} para ti."
                else:
                    message_text = f" 隆Claro! Te abro el mapa con los {location_name} m谩s cercanos."
                
                # Retornar respuesta estructurada
                import json
                return json.dumps({
                    'text': message_text,
                    'deep_link': deep_link,
                    'tool_used': 'open_map_location',
                    'details': {
                        'location_type': intent['location_type'],
                        'search_query': intent['search_query']
                    }
                }, ensure_ascii=False)
            
            # 3. SI NO ES UBICACIN, CONTINUAR CON FLUJO NORMAL DE CHAT
            if chat_history is None:
                chat_history = []
            if relevant_docs is None:
                relevant_docs = []
            
            # Construir historial de chat para contexto
            history_context = ""
            if chat_history:
                history_items = []
                for h in chat_history[-5:]:  # ltimos 5 mensajes
                    history_items.append(f"Usuario: {h.get('message', '')}")
                    history_items.append(f"Asistente: {h.get('response', '')}")
                history_context = "\n\n".join(history_items)
            
            user_info = ""
            if user_context:
                user_info = f"""**Informaci贸n del Usuario:**
- Nombre: {user_context.get('name', 'Usuario')}
- Email: {user_context.get('email', 'No disponible')}
- Actividad: {user_context.get('actividad', 'No especificada')}"""
                
                if user_context.get('ingresos_anuales'):
                    user_info += f"\n- Ingresos anuales: ${user_context['ingresos_anuales']:,}"
                    
                if user_context.get('estado'):
                    user_info += f"\n- Estado: {user_context['estado']}"
            
            docs_context = ""
            if relevant_docs:
                docs_list = []
                for doc in relevant_docs:
                    content = doc.get('content', '')[:200]
                    docs_list.append(f"- {doc.get('title', 'Documento')}: {content}...")
                docs_context = f"**Documentos de Referencia:**\n" + "\n".join(docs_list)
            
            prompt = f"""
{SYSTEM_PROMPT}

{user_info}

{f"**Conversaci贸n Previa:**\n{history_context}" if history_context else ""}

{docs_context}

**Pregunta Actual del Usuario:**
{message}

**Instrucciones:**
- Responde en espa帽ol de manera clara y profesional
- Usa ejemplos pr谩cticos y espec铆ficos para M茅xico
- Si mencionas montos o fechas, s茅 espec铆fico
- Usa **negrita** para conceptos importantes
- Usa *cursiva* para notas adicionales
- Si no tienes suficiente informaci贸n, pregunta amablemente
- Mant茅n un tono amigable pero profesional
- Si la pregunta requiere informaci贸n personal del usuario que no tienes, p铆dela

**IMPORTANTE - NO MENCIONES DETALLES TCNICOS:**
- NUNCA menciones "chunks", "fragmentos", "document IDs", "UUID", o identificadores t茅cnicos
- NO hagas referencia a "Chunk 1, 2, 3" o "documento 0185c8b6..."
- Si citas documentos de referencia, simplemente di: "Seg煤n la informaci贸n disponible..." o "De acuerdo con los documentos..."
- Presenta la informaci贸n de manera natural, como si fuera tu propio conocimiento
- El usuario no necesita saber c贸mo se almacena o estructura la informaci贸n internamente

Responde de manera concisa pero completa:
"""

            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            # Retornar respuesta simple de chat
            import json
            return json.dumps({
                'text': response.text,
                'deep_link': None,
                'tool_used': 'chat',
                'details': {}
            }, ensure_ascii=False)
            
        except Exception as error:
            print(f"Error en chat con asistente: {error}")
            import json
            return json.dumps({
                'text': f"Lo siento, hubo un error al procesar tu mensaje: {str(error)}",
                'deep_link': None,
                'tool_used': 'error',
                'details': {'error': str(error)}
            }, ensure_ascii=False)
    
    async def analyze_fiscal_risk(self, profile: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analizar perfil fiscal y calcular riesgo
        
        Args:
            profile: Perfil fiscal del usuario
            
        Returns:
            An谩lisis de riesgo
        """
        try:
            prompt = f"""
Analiza el siguiente perfil fiscal y calcula un nivel de riesgo:

**Perfil:**
{profile}

Responde en JSON con este formato exacto:
{{
  "score": <n煤mero del 0 al 100, donde 0 es sin riesgo y 100 es alto riesgo>,
  "level": "<Verde|Amarillo|Rojo>",
  "message": "<mensaje breve del estado>",
  "details": {{
    "has_rfc": <boolean>,
    "has_efirma": <boolean>,
    "emite_cfdi": <boolean>,
    "declara_mensual": <boolean>
  }},
  "recommendations": [
    "<lista de recomendaciones espec铆ficas>"
  ]
}}

Criterios de evaluaci贸n:
- Verde (0-30): Cumplimiento fiscal 贸ptimo
- Amarillo (31-60): Requiere atenci贸n en algunas 谩reas
- Rojo (61-100): Alto riesgo fiscal, acci贸n inmediata requerida
"""

            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            text = response.text
            
            # Extraer JSON de la respuesta
            import json
            import re
            
            json_match = re.search(r'\{[\s\S]*\}', text)
            if json_match:
                return json.loads(json_match.group(0))
            
            raise ValueError("No se pudo parsear la respuesta de an谩lisis de riesgo")
            
        except Exception as error:
            print(f"Error analizando riesgo fiscal: {error}")
            # Respuesta por defecto en caso de error
            return {
                'score': 0,
                'level': 'Verde',
                'message': 'An谩lisis no disponible',
                'details': {
                    'has_rfc': profile.get('has_rfc', False),
                    'has_efirma': profile.get('has_efirma', False),
                    'emite_cfdi': profile.get('emite_cfdi', False),
                    'declara_mensual': profile.get('declara_mensual', False)
                },
                'recommendations': []
            }
    
    async def analyze_conversation_for_context_update(
        self,
        current_context: str,
        conversation_messages: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Analiza la conversaci贸n del usuario y determina si el contexto debe actualizarse.
        
        Args:
            current_context: Contexto actual del usuario (puede ser texto o JSON string)
            conversation_messages: Lista de mensajes de la conversaci贸n con 'content', 'created_at', 'user_id'
            
        Returns:
            Dict con should_update (bool), new_context (str), y reasons (list)
        """
        try:
            # Construir el historial de conversaci贸n
            conversation_text = ""
            for msg in conversation_messages:
                # Determinar si es del usuario o del asistente (si user_id es None, es del asistente)
                sender = "Usuario" if msg.get('user_id') else "Asistente"
                content = msg.get('content', '')
                timestamp = msg.get('created_at', '')
                conversation_text += f"\n[{timestamp}] {sender}: {content}"
            
            prompt = f"""Eres un asistente educativo que analiza conversaciones para personalizar la experiencia del usuario.

**CONTEXTO ACTUAL DEL USUARIO:**
{current_context if current_context else "No hay contexto previo"}

**CONVERSACIN COMPLETA:**
{conversation_text}

**INSTRUCCIONES:**
Analiza la conversaci贸n y determina si hay informaci贸n valiosa para actualizar el contexto del usuario.

Busca informaci贸n sobre:
1. **Nivel educativo**: Grado, carrera, 谩rea de estudio
2. **Estilo de aprendizaje**: Visual, auditivo, kinest茅sico, preferencias de explicaci贸n
3. **Intereses acad茅micos**: Temas que le interesan, 谩reas de especializaci贸n
4. **Fortalezas y debilidades**: Materias en las que es fuerte o necesita ayuda
5. **Objetivos educativos**: Metas de aprendizaje, proyectos, ex谩menes
6. **Preferencias de comunicaci贸n**: C贸mo prefiere que le expliquen, nivel de detalle
7. **Horarios o h谩bitos**: Cu谩ndo estudia, con qu茅 frecuencia
8. **Contexto personal relevante**: Cualquier dato que ayude a personalizar el aprendizaje

**RESPONDE EN JSON CON ESTE FORMATO:**
{{
  "should_update": true/false,
  "new_context": "Contexto actualizado en formato de texto descriptivo. Combina el contexto anterior con la nueva informaci贸n encontrada.",
  "reasons": [
    "Raz贸n 1 por la que se debe actualizar",
    "Raz贸n 2..."
  ],
  "key_findings": {{
    "nivel_educativo": "...",
    "estilo_aprendizaje": "...",
    "intereses": ["tema1", "tema2"],
    "fortalezas": ["..."],
    "debilidades": ["..."],
    "objetivos": "...",
    "preferencias": "...",
    "otros": "..."
  }}
}}

**CRITERIOS PARA ACTUALIZAR:**
- Si encuentras informaci贸n nueva y relevante que no est谩 en el contexto actual
- Si hay cambios en el nivel educativo, intereses o objetivos
- Si identificas patrones de aprendizaje o preferencias claras
- Si hay informaci贸n contradictoria que necesita correcci贸n

**NO ACTUALICES SI:**
- La conversaci贸n es muy breve o superficial
- No hay informaci贸n personal o educativa relevante
- La informaci贸n ya est谩 en el contexto actual

Responde SOLO con el JSON:"""

            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            text = response.text.strip()
            
            # Extraer JSON de la respuesta
            import json
            import re
            
            # Limpiar markdown si existe
            if text.startswith("```json"):
                text = text[7:]
            if text.startswith("```"):
                text = text[3:]
            if text.endswith("```"):
                text = text[:-3]
            text = text.strip()
            
            json_match = re.search(r'\{[\s\S]*\}', text)
            if json_match:
                result = json.loads(json_match.group(0))
                return result
            
            raise ValueError("No se pudo parsear la respuesta de an谩lisis de contexto")
            
        except Exception as error:
            print(f"Error analizando conversaci贸n para actualizar contexto: {error}")
            import traceback
            traceback.print_exc()
            return {
                'should_update': False,
                'new_context': current_context,
                'reasons': [f"Error en el an谩lisis: {str(error)}"],
                'key_findings': {}
            }


# Instancia global del cliente
gemini_client = GeminiClient()